# -*- coding: utf-8 -*-
"""decisiontree assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qZKwO9WRKlcFufBCSKML71Mhf0w3tovA
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing the required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing

# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv('/content/adult_dataset-1.csv')
df.head()

df.info

# rows with missing values represented as'?'.
df_1 = df[df.workclass == '?']

# print missing rows
df_1

# Print df_1 varaible information
df_1.info()

# dropping the rows having missing values in workclass
df = df[df.workclass != '?']

# Print first 5 rows of dataframe after dropping
df.head()

# select all categorical variables
df_categorical = df.select_dtypes(include='object')

# checking whether any other columns contain a "?"
df_categorical.apply(lambda x: x=='?', axis=0).sum()

# dropping the "?"s
df = df[df['occupation'] != '?']
df = df[df['native.country'] != '?']

# clean dataframe details
df.info()

# select all categorical variables
df_categorical = df.select_dtypes(include=['object'])

# print first 5 rows of categorical columns
df_categorical.head()

# Initialise LabelEncoder function
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

# Apply encoding on df_categorical
df_categorical = df_categorical.apply(le.fit_transform)


# print df_categorical first 5 rows
df_categorical.head()

# drop all categorical columns from df
df = df.drop(df_categorical.columns, axis=1)

# concat df_categorical to df
df = pd.concat([df, df_categorical], axis=1)

# Print last 5 rows of df
df.tail()

# look at column types in df
df.info()

# convert target variable income to categorical
df['income'] = df['income'].astype('category')

# Importing train-test-split
from sklearn.model_selection import train_test_split

# Putting feature variable to X
X = df.drop('income', axis=1)

# Putting response variable to y
y = df['income']

# Splitting the data into train and test with test size as 30%, and random state as 99|
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)

# Print X_train last 5 rows
X_train.tail()

# Importing decision tree classifier from sklearn library
from sklearn.tree import DecisionTreeClassifier

# Initialise decision tree with default hyperparameters, apart from max_depth which is 5 so that we can plot and read the tree.
dt_default = DecisionTreeClassifier(max_depth=5)

# fitting descision tree on train & test data
dt_default.fit(X_train, y_train)

#Importing classification report and confusion matrix from sklearn metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score


# Making predictions
y_pred_default = dt_default.predict(X_test)

# Printing classification report
print(classification_report(y_test, y_pred_default))

# Printing confusion matrix
print(confusion_matrix(y_test, y_pred_default))

# Printing accuracy
print(accuracy_score(y_test, y_pred_default))

# Importing required packages for visualization
from sklearn import tree

# Putting all feature names in a list
features = list(df.columns[1:])

# Putting all classes in list i.e >=50 and <=50
cls = [">=50", "<=50"]

# Printing features names
print(features)
print("-----------------------------")
# Printing classes names
print(cls)

# Plotting our decision tree for max_depth=3
plt.figure(figsize=(25,10))

plt.tight_layout()
a = tree.plot_tree(dt_default,
                   feature_names=features,
                   class_names=["0", "1"],
                   filled=True,
                   rounded=True,
                   fontsize=14)

# Print textual representation of a tree
text_representation = tree.export_text(dt_default)

# Print text_representation
print(text_representation)

# Import Kfold and GridSearchCV
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV


# specify number of folds for k-fold CV (we are taking 5)
n_folds = 5

# parameters to build the model on ( As of now just max_depth from range 1 to 40)
parameters = {'max_depth':range(1,40)}

# instantiate the model (DecisionTreeClassifier) with criteria gini and random_state as 100
dtree = DecisionTreeClassifier(criterion='gini', random_state=100)

# instantiate GridSearchCV with above 3 parameters and scoring will be accuracy, while return_train_score as true
tree = GridSearchCV(dtree, parameters, cv=n_folds, scoring='accuracy', return_train_score=True)

# fit the tree on training data
tree.fit(X_train, y_train)

# scores of GridSearch CV
scores = tree.cv_results_

# Check the scores in a dataframe  (here just printing first 5 rows)
pd.DataFrame(scores).head()

# plotting accuracies with max_depth vs mean train and test scores
plt.figure()
plt.plot(scores["param_max_depth"], scores["mean_train_score"], label="training accuracy")
plt.plot(scores["param_max_depth"], scores["mean_test_score"], label="test accuracy")
plt.xlabel("max_depth")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# GridSearchCV to find optimal max_depth


# specify number of folds for k-fold CV (here 5)
n_folds = 5

# parameters to build the model on( just min_samples_leaf with range 5 to 200 and with step of 20)
parameters = {'min_samples_leaf':range(5, 200, 20)}

# instantiate the model (DecisionTreeClassifier)
dtree = DecisionTreeClassifier(criterion='gini', random_state=100)

# instantiate GridSearchCV with above 3 parameters and scoring will be accuracy, while return_train_score as true
tree = GridSearchCV(dtree, parameters, cv=n_folds, scoring='accuracy', return_train_score=True)

# fitting the above tree on training data
tree.fit(X_train, y_train)

# scores of GridSearch CV
scores = tree.cv_results_

# Check the scores in a dataframe  (here just printing first 5 rows)
pd.DataFrame(scores).head()

# plotting accuracies with min_samples_leaf vs mean train and test accuracy
plt.figure()
plt.plot(scores["param_min_samples_leaf"], scores["mean_train_score"], label="training accuracy")
plt.plot(scores["param_min_samples_leaf"], scores["mean_test_score"], label="test accuracy")
plt.xlabel("min_samples_leaf")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on (here just min_samples_split with range 5 to 200 with step size as 20)
parameters = {'min_samples_split':range(5, 200, 20)}

# instantiate the model(DecisionTreeClassifier)
dtree = DecisionTreeClassifier(criterion='gini', random_state=100)

# instantiate GridSearchCV with above 3 parameters and scoring will be accuracy, while return_train_score as true
tree = GridSearchCV(dtree, parameters, cv=n_folds, scoring='accuracy', return_train_score=True)

# fitting the above tree on training data
tree.fit(X_train, y_train)

# scores of GridSearch CV
scores = tree.cv_results_

# Check the scores in a dataframe  (here just printing first 5 rows)
pd.DataFrame(scores).head()

# plotting accuracies with min_samples_leaf  vs mean train and test accuracy
plt.figure()
plt.plot(scores["param_min_samples_split"], scores["mean_train_score"], label="training accuracy")
plt.plot(scores["param_min_samples_split"], scores["mean_test_score"], label="test accuracy")
plt.xlabel("min_samples_split")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Create the parameter grid ( we already defined for you)
param_grid = {
    'max_depth': range(5, 15, 5),
    'min_samples_leaf': range(50, 150, 50),
    'min_samples_split': range(50, 150, 50),
    'criterion': ["entropy", "gini"]
}

# no. of folds(5)
n_folds = 5

# Instantiate the grid search model with default parameters value
dtree = DecisionTreeClassifier(random_state=42)


# instantiate GridSearchCV with above 3 parameters and verbose as 1
grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=n_folds, verbose=1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# cv results
cv_results = pd.DataFrame(grid_search.cv_results_)

# Printing CV_result
cv_results

# printing the optimal accuracy score
print("Best Accuracy", grid_search.best_score_)

# Printing best hyperparameters parameters & its values
print(grid_search.best_params_)

# model with optimal hyperparameters also add random state as 100
clf_gini = DecisionTreeClassifier(criterion='gini',
                                  random_state=100,
                                  max_depth=10,
                                  min_samples_leaf=50,
                                  min_samples_split=50)


# fitting above model with training data/
clf_gini.fit(X_train, y_train)

# accuracy score
clf_gini.score(X_test, y_test)

# Importing tree for visualization
from sklearn import tree

# plotting the tree
plt.figure(figsize=(25,10))

plt.tight_layout()
a = tree.plot_tree(clf_gini,
                   feature_names=features,
                   class_names=["0", "1"],
                   filled=True,
                   rounded=True,
                   fontsize=14)

# tree with max_depth = 3
clf_gini = DecisionTreeClassifier(criterion='gini',
                                  random_state=100,
                                  max_depth=3,
                                  min_samples_leaf=50,
                                  min_samples_split=50)


# fitting the model with training data
clf_gini.fit(X_train, y_train)

# Print model score
print(clf_gini.score(X_test, y_test))

# plotting tree with max_depth=3
plt.figure(figsize=(25,10))

plt.tight_layout()
a = tree.plot_tree(clf_gini,
                   feature_names=features,
                   class_names=["0", "1"],
                   filled=True,
                   rounded=True,
                   fontsize=14)

# Predict on test data
y_pred = clf_gini.predict(X_test)

# print matrix of classification_report
print(classification_report(y_test, y_pred))

#Initialise cost_complexity_pruning_path with training data
path = clf_gini.cost_complexity_pruning_path(X_train, y_train)

# Checing out ccp_alphas and impurities
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# Printing all alphas
ccp_alphas

# Initialise empty arrays for train and test accuracy
Accuracy_Train, Accuracy_Test = [], []

# Iterate over ccp_alpha
for ccp_alpha in ccp_alphas:

    # Instantiate DecisionTreeClassifier with random_state=0 and ccp_alpha as ccp_alpha iterated value
    clf = DecisionTreeClassifier(random_state=100, ccp_alpha=ccp_alpha)

    # Fitting model on training data
    clf.fit(X_train, y_train)

    # Predicting on train data
    Train_pred = clf.predict(X_train)

    # predicting on test data
    Test_pred = clf.predict(X_test)

    # Appending training accuracy
    Accuracy_Train.append(accuracy_score(y_train, Train_pred))

    # Appending test accuracy
    Accuracy_Test.append(accuracy_score(y_test, Test_pred))

# Plotting all alpha vs Train and test accuracy scores
train_scores = Accuracy_Train
test_scores = Accuracy_Test

plt.figure(figsize=(10,4))

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy Vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test", drawstyle="steps-post")
ax.legend()
plt.show()

# Instantiate DecisionTreeClassifier
final_dt = DecisionTreeClassifier(ccp_alpha=0.01, random_state=100,
                                  max_depth=3,
                                  min_samples_leaf=50,
                                  min_samples_split=50)

# Fitting model on training data
final_dt.fit(X_train, y_train)

# Predicting on train data
Train_pred = clf.predict(X_train)

# predicting on test data
Test_pred = clf.predict(X_test)

# Print final test and train accuracy
print(accuracy_score(y_train, Train_pred))
print(accuracy_score(y_test, Test_pred))

# plotting tree.

plt.figure(figsize=(25,10))

plt.tight_layout()
a = tree.plot_tree(clf_gini,
                   feature_names=features,
                   class_names=["0", "1"],
                   filled=True,
                   rounded=True,
                   fontsize=14)

