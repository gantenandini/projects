# -*- coding: utf-8 -*-
"""blackcoffer.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12cB2XSj0Po1eqsXSUX_HrXSm_03WaaP9
"""

#import necessary pacakages
import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
import re

#read the url file into the pandas object
df = pd.read_excel('Input.xlsx')
df

#loop throgh each row in the df
for index, row in df.iterrows():
  url = row['URL']
  url_id = row['URL_ID']

  # make a request to url
  header = {'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"}
  try:
    response = requests.get(url,headers=header)
  except:
    print("can't get response of {}".format(url_id))

  #create a beautifulsoup object
  try:
    soup = BeautifulSoup(response.content, 'html.parser')
  except:
    print("can't get page of {}".format(url_id))
  #find title
  try:
    title = soup.find('h1').get_text()
  except:
    print("can't get title of {}".format(url_id))
    continue
  #find text
  article = ""
  try:
    for p in soup.find_all('p'):
      article += p.get_text()
  except:
    print("can't get text of {}".format(url_id))

  #write title and text to the file
  file_name = '/content/drive/MyDrive/assignment/text' + str(url_id) + '.txt'
  with open(file_name, 'w') as file:
    file.write(title + '\n' + article)

text_dir= '/content/drive/MyDrive/assignment'
stopwords_dir = '/content/drive/MyDrive/AGANITHA/stopwords'
sentment_dir = '/content/drive/MyDrive/AGANITHA/sentiment'

# load all stop wors from the stopwords directory and store in the set variable
stop_words = set()
for files in os.listdir(stopwords_dir):
  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:
    stop_words.update(set(f.read().splitlines()))

# load all text files  from the  directory and store in a list(docs)
docs = []
for text_file in os.listdir(text_dir):
  with open(os.path.join(text_dir,text_file),'r') as f:
    text = f.read()
#tokenize the given text file
    words = word_tokenize(text)
# remove the stop words from the tokens
    filtered_text = [word for word in words if word.lower() not in stop_words]
# add each filtered tokens of each file into a list
    docs.append(filtered_text)



# store positive, Negative words from the directory
pos=set()
neg=set()

for files in os.listdir(sentment_dir):
  if files =='positive-words.txt':
    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:
      pos.update(f.read().splitlines())
  else:
    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:
      neg.update(f.read().splitlines())

# now collect the positive  and negative words from each file
# calculate the scores from the positive and negative words
positive_words = []
Negative_words =[]
positive_score = []
negative_score = []
polarity_score = []
subjectivity_score = []

#iterate through the list of docs
for i in range(len(docs)):
  positive_words.append([word for word in docs[i] if word.lower() in pos])
  Negative_words.append([word for word in docs[i] if word.lower() in neg])
  positive_score.append(len(positive_words[i]))
  negative_score.append(len(Negative_words[i]))
  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))
  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))

print(positive_words)
print(Negative_words)
print(positive_score)
print(negative_score)
print(polarity_score)
print(subjectivity_score)

from google.colab import drive

# Mount Google Drive to Colab
drive.mount('/content/drive')

# Average Sentence Length = the number of words / the number of sentences
# Percentage of Complex words = the number of complex words / the number of words
# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)

avg_sentence_length = []
Percentage_of_Complex_words  =  []
Fog_Index = []
complex_word_count =  []
avg_syllable_word_count =[]

stopwords = set(stopwords.words('english'))
def measure(file):
  with open(os.path.join(text_dir, file),'r') as f:
    text = f.read()
# remove punctuations
    text = re.sub(r'[^\w\s.]','',text)
# split the given text file into sentences
    sentences = text.split('.')
# total number of sentences in a file
    num_sentences = len(sentences)
# total words in the file
    words = [word  for word in text.split() if word.lower() not in stopwords ]
    num_words = len(words)

# complex words having syllable count is greater than 2
# Complex words are words in the text that contain more than two syllables.
    complex_words = []
    for word in words:
      vowels = 'aeiou'
      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
      if syllable_count_word > 2:
        complex_words.append(word)


    syllable_count = 0
    syllable_words =[]
    for word in words:
      if word.endswith('es'):
        word = word[:-2]
      elif word.endswith('ed'):
        word = word[:-2]
      vowels = 'aeiou'
      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
      if syllable_count_word >= 1:
        syllable_words.append(word)
        syllable_count += syllable_count_word



    avg_sentence_len = num_words / num_sentences
    avg_syllable_word_count = syllable_count / len(syllable_words)
    Percentage_of_Complex_words  =  len(complex_words) / num_words
    Fog_Index = 0.4 * (avg_sentence_len + Percentage_of_Complex_words)

    return avg_sentence_len, Percentage_of_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count

# iterate through each file or doc
for file in os.listdir(text_dir):
  x,y,z,a,b = measure(file)
  avg_sentence_length.append(x)
  Percentage_of_Complex_words.append(y)
  Fog_Index.append(z)
  complex_word_count.append(a)
  avg_syllable_word_count.append(b)

print(Fog_Index)
print(avg_syllable_word_count)
print(Percentage_of_Complex_words)
print(avg_sentence_length)
print(complex_word_count)

# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words
# We count the total cleaned words present in the text by
# removing the stop words (using stopwords class of nltk package).
# removing any punctuations like ? ! , . from the word before counting.

def cleaned_words(file):
  with open(os.path.join(text_dir,file), 'r') as f:
    text = f.read()
    text = re.sub(r'[^\w\s]', '' , text)
    words = [word  for word in text.split() if word.lower() not in stopwords]
    length = sum(len(word) for word in words)
    average_word_length = length / len(words)
  return len(words),average_word_length

word_count = []
average_word_length = []
for file in os.listdir(text_dir):
  x, y = cleaned_words(file)
  word_count.append(x)
  average_word_length.append(y)

# To calculate Personal Pronouns mentioned in the text, we use regex to find
# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken
#  so that the country name US is not included in the list.
def count_personal_pronouns(file):
  with open(os.path.join(text_dir,file), 'r') as f:
    text = f.read()
    personal_pronouns = ["I", "we", "my", "ours", "us"]
    count = 0
    for pronoun in personal_pronouns:
      count += len(re.findall(r"\b" + pronoun + r"\b", text)) # \b is used to match word boundaries
  return count

pp_count = []
for file in os.listdir(text_dir):
  x = count_personal_pronouns(file)
  pp_count.append(x)

print(word_count)
print(average_word_length)
print(pp_count)

data = {
    'POSITIVE SCORE': positive_score,
    'NEGATIVE SCORE': negative_score,
    'POLARITY SCORE': polarity_score,
    'SUBJECTIVITY SCORE': subjectivity_score,
    'AVG SENTENCE LENGTH': avg_sentence_length,
    'PERCENTAGE OF COMPLEX WORDS': Percentage_of_Complex_words,
    'FOG INDEX': Fog_Index,
    'AVG NUMBER OF WORDS PER SENTENCE': avg_syllable_word_count,
    'COMPLEX WORD COUNT': complex_word_count,
    'WORD COUNT': word_count,
    'PERSONAL PRONOUNS': pp_count,
    'AVG WORD LENGTH': average_word_length
}

# Create a DataFrame from the data
df = pd.DataFrame(data)
df

excel_file_path = 'output.xlsx'

# Save the DataFrame to an Excel file
df.to_excel(excel_file_path, sheet_name='Final Results', index=False,encoding='utf-16')

print(f"Excel file '{excel_file_path}' has been created.")

# Load the data from 'text_analysis_results.xlsx'
df2 = pd.read_excel('output.xlsx')

# Load the data from 'input.xlsx'
df1 = pd.read_excel('Input.xlsx')

# Merge the first two columns of df1 with df2
merged_df = pd.concat([df1.iloc[:, :2], df2], axis=1)

# Save the merged DataFrame to a new Excel file
merged_df.to_excel('Final Results.xlsx', index=False)

print("Success.")

import os
import re

text_dir = '/content/drive/MyDrive/assignment'

def count_syllables(word):
    # Define a regular expression pattern to find vowel sequences
    pattern = re.compile(r'[aeiouy]+', re.IGNORECASE)
    vowel_sequences = pattern.findall(word)

    # Handle exceptions for words ending with "es" or "ed"
    if word.endswith(('es', 'ed')):
        # Exclude the last sequence if it's "es" or "ed" to avoid double counting
        if vowel_sequences and vowel_sequences[-1].lower() in ('es', 'ed'):
            vowel_sequences.pop()

    return len(vowel_sequences)

for filename in os.listdir(text_dir):
  if filename.endswith('.txt'):
        file_path = os.path.join(text_dir, filename)

        with open(file_path, 'r', encoding='utf-8') as file:
            cleaned_text = file.read()

        words = cleaned_text.split()

        syllable_count = sum(count_syllables(word) for word in words)

        average_syllables_per_word = syllable_count / len(words)


        print(average_syllables_per_word)

import os
import re

text_dir = '/content/drive/MyDrive/assignment'

def count_syllables(word):
    # Define a regular expression pattern to find vowel sequences
    pattern = re.compile(r'[aeiou]+', re.IGNORECASE)
    vowel_sequences = pattern.findall(word)

    # Handle exceptions for words ending with "es" or "ed"
    if word.endswith(('es', 'ed')):
        # Exclude the last sequence if it's "es" or "ed" to avoid double counting
        if vowel_sequences and vowel_sequences[-1].lower() in ('es', 'ed'):
            vowel_sequences.pop()

    return len(vowel_sequences)

for filename in os.listdir(text_dir):
    if filename.endswith('.txt'):
        file_path = os.path.join(text_dir, filename)

        with open(file_path, 'r', encoding='utf-8') as file:
            cleaned_text = file.read()

        words = cleaned_text.split()

        syllable_counts_list = [count_syllables(word) for word in words]

        print(syllable_counts_list)

